{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experimented with a few models that performed similar in vaccine_train.ipynb. \n",
    "- Use Linear SVC to predict\n",
    "- By Xiaoyi Yuan\n",
    "- Aug 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move the label to the full dataset\n",
    "- I made the mistake when seperating data for labeling and the rest\n",
    "- I should have stored them in seperate files\n",
    "- Now I have to find in the full dataset which are labeled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows=100\n",
    "pd.options.display.max_colwidth=200\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of rows is:  660983\n",
      "the number of column is:  21\n"
     ]
    }
   ],
   "source": [
    "fulldata = pd.read_csv(\"../data/vaccination_all_wrt_full.tsv\",delimiter = '\\t', dtype=str)\n",
    "# delete duplicates:\n",
    "fulldata= fulldata.drop_duplicates(keep=\"first\")\n",
    "print(\"the number of rows is: \",len(fulldata))\n",
    "print(\"the number of column is: \", len(fulldata.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "41\n",
      "805\n",
      "598\n"
     ]
    }
   ],
   "source": [
    "# read the sample data (those for labeling)\n",
    "#unfortunately the retweeted_id and response id here are all wrong:\n",
    "\n",
    "data_xy=pd.read_csv(\"sample_xy_08_31.csv\",dtype=str)\n",
    "data_ja=pd.read_csv(\"sample_ja_08_31.csv\",dtype=str)\n",
    "n_read_xy=855\n",
    "n_read_ja=639\n",
    "\n",
    "# those we read\n",
    "read_xy = data_xy[:n_read_xy]\n",
    "read_ja = data_ja[:n_read_ja]\n",
    "\n",
    "# those we read but did not label\n",
    "delete_xy = read_xy[read_xy['label'].isnull()]\n",
    "delete_ja = read_ja[read_ja['label'].isnull()]\n",
    "\n",
    "# those we read but labeled\n",
    "labeled_xy = read_xy[~read_xy['label'].isnull()]\n",
    "labeled_ja = read_ja[~read_ja['label'].isnull()]\n",
    "\n",
    "print(len(delete_xy))\n",
    "print(len(delete_ja))\n",
    "\n",
    "print(len(labeled_xy))\n",
    "print(len(labeled_ja))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete read but unlabeled ones\n",
    "fulldata = fulldata[~fulldata['id'].isin(delete_xy['id'])]\n",
    "fulldata = fulldata[~fulldata['id'].isin(delete_ja['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add the labels to the fulldata set by matching id\n",
    "\n",
    "fulldata_xy = fulldata [fulldata['id'].isin(labeled_xy['id'])]\n",
    "fulldata_ja = fulldata [fulldata['id'].isin(labeled_ja['id'])]\n",
    "add_label=fulldata_xy.append(fulldata_ja)\n",
    "\n",
    "labeled=labeled_xy.append(labeled_ja)\n",
    "labeled = labeled[['id','label']]\n",
    "\n",
    "# merge \n",
    "fulldata_add_label = pd.merge(add_label,labeled,on=\"id\",how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now find the labeled_xy and labeled_ja id in the fulldata and add that to fulldata\n",
    "# cannot add labeled_xy/ja directed because all other columns in that data are wrong\n",
    "\n",
    "fulldata_no_label = fulldata [~fulldata['id'].isin(add_label['id'])]\n",
    "fulldata = fulldata_no_label.append(fulldata_add_label).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output the data into a new file (i.e. data with read&unlabeled deleted and labels moved.)\n",
    "fulldata.to_csv(\"fulldata_08_31.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seperate training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"fulldata_08_31.csv\",dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled = data[data['label'].notnull()]\n",
    "unlabeled=data[data['label'].isnull()]\n",
    "\n",
    "labeled.to_csv(\"labeled_08_31.csv\",index=False)\n",
    "unlabeled.to_csv(\"unlabeled_08_31.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare models\n",
    "- vaccination_train.py test different models on previously labeled data\n",
    "- data labels changed since then, so test model again\n",
    "- it's demonstrated earlier (on same data) that linear models perform better, so test linear models first.\n",
    "- the code following is all from vaccination_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "labeled = pd.read_csv(\"labeled_08_31.csv\",dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     640\n",
       "-1    417\n",
       "0     346\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "def clean_text(text):\n",
    "#remove the emoji or other weird content (such as ðŸ‡ºðŸ‡) \n",
    "    text = ''.join(filter(lambda x: x in string.printable, text))\n",
    "#remove urls and @\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(\"RT\",\"\",text)\n",
    "    text = ' '.join(filter(lambda x:x[0]!=\"@\",text.split()))\n",
    "    text = ' '.join(filter(lambda x:x[0]!=\"&\",text.split()))\n",
    "    text= \" \".join(list(map(lambda x:x.strip(\"#\"),text.split()))) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace the text with its cleaned version\n",
    "labeled[\"text\"]=labeled[\"text\"].map(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# before testing models, create a customed tokenizer. \n",
    "import spacy \n",
    "\n",
    "regexp = re.compile(\"(?u)\\\\b\\\\w\\\\w+\\\\b\")\n",
    "en_nlp=spacy.load('en')\n",
    "old_tokenizer=en_nlp.tokenizer\n",
    "en_nlp.tokenizer=lambda string: old_tokenizer.tokens_from_list (regexp.findall(string))\n",
    "\n",
    "def custom_tokenizer(document):\n",
    "    doc_spacy=en_nlp(document,entity=False, parse=False)\n",
    "    return [token.lemma_ for token in doc_spacy]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split data into training (80%) and test data (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(labeled[\"text\"], labeled[\"label\"], test_size=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modeling function\n",
    "def modeling_accuracy (model,param_grid):\n",
    "    vect = TfidfVectorizer(stop_words=\"english\",tokenizer=custom_tokenizer,norm=None)\n",
    "    pipe = make_pipeline(vect,model)\n",
    "    grid = GridSearchCV (pipe, param_grid, cv=StratifiedKFold(n_splits=5),scoring=\"accuracy\")\n",
    "    grid.fit(X_train,y_train)\n",
    "    print(\"the best cv score is: \", grid.best_score_)\n",
    "    print(\"parameters of best cv score are: \", grid.best_params_)\n",
    "    print(\"the score on test set is: \", grid.score(X_test,y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is:  0.694840834248\n",
      "parameters of best cv score are:  {'logisticregression__C': 0.1, 'tfidfvectorizer__ngram_range': (1, 3), 'tfidfvectorizer__min_df': 1}\n",
      "the score on test set is:  0.743902439024\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression on the whole labeled data:\n",
    "\n",
    "param_grid={'logisticregression__C':[0.1,0.01,0.001],\n",
    "           'tfidfvectorizer__ngram_range':[(1,1),(1,2),(1,3)],\n",
    "           'tfidfvectorizer__min_df':[1,2,3,4,5]}\n",
    "\n",
    "modeling_accuracy (LogisticRegression(),param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is:  0.708013172338\n",
      "parameters of best cv score are:  {'tfidfvectorizer__ngram_range': (1, 1), 'linearsvc__C': 0.001, 'tfidfvectorizer__min_df': 1}\n",
      "the score on test set is:  0.723577235772\n"
     ]
    }
   ],
   "source": [
    "param_grid={'linearsvc__C':[1, 0.1,0.01,0.001],\n",
    "           'tfidfvectorizer__ngram_range':[(1,1),(1,2)],\n",
    "           'tfidfvectorizer__min_df':[1,2]}\n",
    "\n",
    "modeling(LinearSVC(),param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non-linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is:  0.665873959572\n",
      "parameters of best cv score are:  {'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__min_df': 2, 'svc__gamma': 0.01, 'svc__C': 10}\n",
      "the score on test set is:  0.61743772242\n"
     ]
    }
   ],
   "source": [
    "param_grid={\n",
    "            'svc__C':[0.01,0.1,1,10],\n",
    "            'svc__gamma':[0.01,0.1,1,10],\n",
    "            'tfidfvectorizer__min_df':[1,2,3],\n",
    "            'tfidfvectorizer__ngram_range':[(1,1)]}\n",
    "\n",
    "modeling (SVC(),param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD + linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is:  0.713501646542\n",
      "parameters of best cv score are:  {'sgdclassifier__alpha': 0.1, 'tfidfvectorizer__ngram_range': (1, 2), 'sgdclassifier__learning_rate': 'optimal', 'sgdclassifier__fit_intercept': True, 'tfidfvectorizer__min_df': 1}\n",
      "the score on test set is:  0.70325203252\n"
     ]
    }
   ],
   "source": [
    "param_grid={\n",
    "    \"tfidfvectorizer__ngram_range\":[(1,1),(1,2)],\n",
    "    'tfidfvectorizer__min_df':[1],\n",
    "    \"sgdclassifier__alpha\":[1,0.1,0.01,0.001],\n",
    "    \"sgdclassifier__fit_intercept\":[True],\n",
    "    \"sgdclassifier__learning_rate\":['optimal']}\n",
    "\n",
    "modeling (SGDClassifier(),param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD + logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is:  0.702734839477\n",
      "parameters of best cv score are:  {'sgdclassifier__alpha': 0.1, 'sgdclassifier__loss': 'log', 'tfidfvectorizer__ngram_range': (1, 2), 'sgdclassifier__learning_rate': 'optimal', 'sgdclassifier__fit_intercept': True, 'tfidfvectorizer__min_df': 1}\n",
      "the score on test set is:  0.681494661922\n"
     ]
    }
   ],
   "source": [
    "param_grid={\n",
    "    \"tfidfvectorizer__ngram_range\":[(1,1),(1,2),(1,3)],\n",
    "    'tfidfvectorizer__min_df':[1,2,3,4,5],\n",
    "    \"sgdclassifier__alpha\":[1,0.1,0.01,0.001],\n",
    "    \"sgdclassifier__fit_intercept\":[True],\n",
    "    \"sgdclassifier__learning_rate\":['optimal'],\n",
    "    \"sgdclassifier__loss\":['log']}\n",
    "\n",
    "modeling(SGDClassifier(),param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict\n",
    "- linear SVC and SGD+SVC performance is very similar but I choose linear SVC because the parameters are not as sensitive and it's more interpretable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import labeled and unlabeled data\n",
    "\n",
    "labeled = pd.read_csv(\"labeled_08_31.csv\",dtype=str)\n",
    "unlabeled = pd.read_csv(\"unlabeled_08_31.csv\",dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos=labeled[labeled['label']=='1']\n",
    "neg=labeled[labeled['label']=='-1']\n",
    "neu=labeled[labeled['label']=='0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640\n",
      "417\n",
      "346\n"
     ]
    }
   ],
   "source": [
    "print(len(pos))\n",
    "print(len(neg))\n",
    "print(len(neu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629\n",
      "303\n",
      "341\n"
     ]
    }
   ],
   "source": [
    "print(len(pos.groupby('author')))\n",
    "print(len(neg.groupby('author')))\n",
    "print(len(neu.groupby('author')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1253"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled.groupby('author'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "659489"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_text=labeled[\"text\"]\n",
    "target=labeled[\"label\"]\n",
    "unlabeled_text=unlabeled[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean \n",
    "labeled_text=labeled_text.map(clean_text)\n",
    "unlabeled_text=unlabeled_text.map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorize use the best parameter from experiments in linear SVC, i.e. n_gram and min_df\n",
    "vect = TfidfVectorizer(stop_words=\"english\",tokenizer=custom_tokenizer,norm=None, \n",
    "                                   ngram_range=(1,1),min_df=1)\n",
    "vect = vect.fit(labeled_text)\n",
    "labeled_vect = vect.transform(labeled_text)\n",
    "unlabeled_vect = vect.transform(unlabeled_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.001, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearSVC(C=0.001)\n",
    "model.fit(labeled_vect,target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "213px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "249px",
    "left": "979.9890747070313px",
    "right": "20px",
    "top": "-1.0054349899291992px",
    "width": "267px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
