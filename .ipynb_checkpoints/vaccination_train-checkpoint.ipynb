{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training vaccine twitter data: broadly testing different classifiers and parameters without final prediction.\n",
    "- by Xiaoyi Yuan,\n",
    "- July 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows=100\n",
    "pd.options.display.max_colwidth=200\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path =\"/Users/Charlotte/Google Drive/Jaxy Project/training\"\n",
    "data_xy=pd.read_csv(os.path.join(path,\"sample_xy_07_28.csv\"))\n",
    "data_ja=pd.read_csv(os.path.join(path,\"sample_ja_07_28.csv\"))\n",
    "n_labeled_xy=807\n",
    "n_labeled_ja=630"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN     29220\n",
      " 0.0      302\n",
      " 1.0      271\n",
      "-1.0      207\n",
      "Name: label, dtype: int64\n",
      "NaN     29397\n",
      " 0.0      240\n",
      " 1.0      213\n",
      "-1.0      150\n",
      "Name: label, dtype: int64\n",
      "False    780\n",
      "True      27\n",
      "Name: label, dtype: int64\n",
      "False    603\n",
      "True      27\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#double check the number for each label. \n",
    "\n",
    "dfv_xy=data_xy['label'].value_counts(dropna=False)\n",
    "dfv_ja=data_ja['label'].value_counts(dropna=False)\n",
    "\n",
    "#check how many data is being read but not labeled (skipped data)\n",
    "null_xy=data_xy[:n_labeled_xy][\"label\"].isnull().value_counts()\n",
    "null_ja=data_ja[:n_labeled_ja][\"label\"].isnull().value_counts()\n",
    "\n",
    "print(dfv_xy)\n",
    "print(dfv_ja)\n",
    "print(null_xy)\n",
    "print(null_ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the labeled data\n",
    "data_xy=data_xy[pd.notnull(data_xy[\"label\"])]\n",
    "data_ja=data_ja[pd.notnull(data_ja[\"label\"])]\n",
    "\n",
    "#Put xy and ja labeled data together\n",
    "data=data_xy.append(data_ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the data with labels of either 1 or -1\n",
    "data_xy_binary=data_xy[data_xy.label!= 0]\n",
    "data_ja_binary=data_ja[data_ja.label!= 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478\n",
      "363\n"
     ]
    }
   ],
   "source": [
    "print(len(data_xy_binary))\n",
    "print(len(data_ja_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_binary=data_xy_binary.append(data_ja_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1383 entries, 0 to 628\n",
      "Data columns (total 22 columns):\n",
      "author          1383 non-null object\n",
      "autism          1383 non-null int64\n",
      "coords_from     752 non-null object\n",
      "country         746 non-null object\n",
      "id              1383 non-null float64\n",
      "label           1383 non-null float64\n",
      "lang            1383 non-null object\n",
      "location        891 non-null object\n",
      "measl           1383 non-null int64\n",
      "mood            1383 non-null int64\n",
      "mump            1383 non-null int64\n",
      "published_at    1383 non-null object\n",
      "response_id     71 non-null float64\n",
      "retweeted_id    608 non-null float64\n",
      "state           601 non-null object\n",
      "text            1383 non-null object\n",
      "vaccin          1383 non-null int64\n",
      "vax             1383 non-null int64\n",
      "vaxin           1383 non-null int64\n",
      "x               752 non-null float64\n",
      "y               752 non-null float64\n",
      "zip             450 non-null object\n",
      "dtypes: float64(6), int64(7), object(9)\n",
      "memory usage: 248.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove url,non-sensical words, emojis and @ (kept the content of hashtags).\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "def clean_text(text):\n",
    "#remove the emoji or other weird content (such as ðŸ‡ºðŸ‡) \n",
    "    text = ''.join(filter(lambda x: x in string.printable, text))\n",
    "#remove urls and @\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(\"RT\",\"\",text)\n",
    "    text = ' '.join(filter(lambda x:x[0]!=\"@\",text.split()))\n",
    "    text = ' '.join(filter(lambda x:x[0]!=\"&\",text.split()))\n",
    "    text= \" \".join(list(map(lambda x:x.strip(\"#\"),text.split()))) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace the text with its cleaned version\n",
    "data[\"text\"]=data[\"text\"].map(clean_text)\n",
    "data_binary[\"text\"]=data_binary[\"text\"].map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndate=datetime.date.today()\\nfile_name = os.path.join(path, str(date) + \".\" + \"csv\")\\ndata.to_csv(file_name)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export a copy using today's date as the file name\n",
    "'''\n",
    "date=datetime.date.today()\n",
    "file_name = os.path.join(path, str(date) + \".\" + \"csv\")\n",
    "data.to_csv(file_name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 1: logistic regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# before testing models, create a customed tokenizer. \n",
    "import spacy \n",
    "\n",
    "regexp = re.compile(\"(?u)\\\\b\\\\w\\\\w+\\\\b\")\n",
    "en_nlp=spacy.load('en')\n",
    "old_tokenizer=en_nlp.tokenizer\n",
    "en_nlp.tokenizer=lambda string: old_tokenizer.tokens_from_list (regexp.findall(string))\n",
    "\n",
    "def custom_tokenizer(document):\n",
    "    doc_spacy=en_nlp(document,entity=False, parse=False)\n",
    "    return [token.lemma_ for token in doc_spacy]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split data into training (80%) and test data (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"label\"], test_size=0.2)\n",
    "X_train_binary,X_test_binary, y_train_binary,y_test_binary=train_test_split(data_binary[\"text\"], data_binary[\"label\"], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is:  0.655515370705\n",
      "parameters of best cv score are:  {'logisticregression__C': 0.01, 'tfidfvectorizer__ngram_range': (1, 2), 'tfidfvectorizer__min_df': 1}\n",
      "the score on test set is:  0.671480144404\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression on the whole labeled data:\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(stop_words=\"english\",tokenizer=custom_tokenizer,norm=None),LogisticRegression())\n",
    "param_grid={'logisticregression__C':[0.1,0.01,0.001],\n",
    "           'tfidfvectorizer__ngram_range':[(1,1),(1,2),(1,3)],\n",
    "           'tfidfvectorizer__min_df':[1,2,3,4,5]}\n",
    "grid=GridSearchCV(pipe, param_grid,cv=StratifiedKFold(n_splits=5),scoring=\"accuracy\")\n",
    "grid.fit(X_train,y_train)\n",
    "print(\"the best cv score is: \", grid.best_score_)\n",
    "print(\"parameters of best cv score are: \", grid.best_params_)\n",
    "print(\"the score on test set is: \", grid.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is:  0.813988095238\n",
      "parameters of best cv score are:  {'logisticregression__C': 0.01, 'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__min_df': 1}\n",
      "the score on test set is:  0.804733727811\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression on binary classifier data (negative and positive)\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(stop_words=\"english\",tokenizer=custom_tokenizer,norm=None),LogisticRegression())\n",
    "param_grid={'logisticregression__C':[0.1,0.01,0.001],\n",
    "           'tfidfvectorizer__ngram_range':[(1,1),(1,2),(1,3)],\n",
    "           'tfidfvectorizer__min_df':[1,2,3,4,5]}\n",
    "grid=GridSearchCV(pipe, param_grid,cv=StratifiedKFold(n_splits=10),scoring=\"accuracy\")\n",
    "grid.fit(X_train_binary,y_train_binary)\n",
    "print(\"the best cv score is: \", grid.best_score_)\n",
    "print(\"parameters of best cv score are: \", grid.best_params_)\n",
    "print(\"the score on test set is: \", grid.score(X_test_binary,y_test_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is:  0.641952983725\n",
      "the score on test set is:  0.631768953069\n"
     ]
    }
   ],
   "source": [
    "# try CountVectorizer instead of TfidfVectorizer and see which one is the best for featuring\n",
    "\n",
    "vect=CountVectorizer(min_df=3, stop_words=\"english\",tokenizer=custom_tokenizer,ngram_range=(1,1)).fit(X_train)\n",
    "X_train=vect.transform(X_train)\n",
    "param_grid={'C':[0.1,0.01,0.001]}\n",
    "grid=GridSearchCV(LogisticRegression(), param_grid,cv=StratifiedKFold(n_splits=10),scoring=\"accuracy\")\n",
    "grid.fit(X_train,y_train)\n",
    "print(\"the best cv score is: \",grid.best_score_)\n",
    "X_test=vect.transform(X_test)\n",
    "print(\"the score on test set is: \",grid.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on the cv score and score on test set, CountVectorizer is not performing as well as TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 2: Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"label\"], test_size=0.2)\n",
    "X_train_binary,X_test_binary, y_train_binary,y_test_binary=train_test_split(data_binary[\"text\"], data_binary[\"label\"], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is:  0.662748643761\n",
      "parameters of best cv score are:  {'tfidfvectorizer__ngram_range': (1, 1), 'linearsvc__C': 0.001, 'tfidfvectorizer__min_df': 1}\n",
      "the score on test set is:  0.689530685921\n"
     ]
    }
   ],
   "source": [
    "#SVC on 3-classifier data\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(stop_words=\"english\",tokenizer=custom_tokenizer,norm=None),LinearSVC())\n",
    "param_grid={'linearsvc__C':[1, 0.1,0.01,0.001],\n",
    "           'tfidfvectorizer__ngram_range':[(1,1),(1,2),(1,3)],\n",
    "           'tfidfvectorizer__min_df':[1,2,3,4,5]}\n",
    "grid =GridSearchCV(pipe, param_grid,cv=StratifiedKFold(n_splits=10),scoring=\"accuracy\")\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "print(\"parameters of best cv score are: \", grid.best_params_)\n",
    "print(\"the score on test set is: \", grid.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is:  0.815476190476\n",
      "parameters of best cv score are:  {'tfidfvectorizer__ngram_range': (1, 2), 'linearsvc__C': 0.001, 'tfidfvectorizer__min_df': 1}\n",
      "the score on test set is:  0.798816568047\n"
     ]
    }
   ],
   "source": [
    "#SVC on 2-classifier data\n",
    "pipe = make_pipeline(TfidfVectorizer(stop_words=\"english\",tokenizer=custom_tokenizer,norm=None),LinearSVC())\n",
    "param_grid={'linearsvc__C':[1, 0.1,0.01,0.001],\n",
    "           'tfidfvectorizer__ngram_range':[(1,1),(1,2),(1,3)],\n",
    "           'tfidfvectorizer__min_df':[1,2,3,4,5]}\n",
    "grid =GridSearchCV(pipe, param_grid,cv=StratifiedKFold(n_splits=10),scoring=\"accuracy\")\n",
    "grid.fit(X_train_binary,y_train_binary)\n",
    "print(\"the best cv score is: \", grid.best_score_)\n",
    "print(\"parameters of best cv score are: \", grid.best_params_)\n",
    "print(\"the score on test set is: \", grid.score(X_test_binary,y_test_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Same here, in LinearSVC model, TfidfVectorizer performs better than CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 3: non-linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"label\"], test_size=0.2)\n",
    "X_train_binary,X_test_binary, y_train_binary,y_test_binary=train_test_split(data_binary[\"text\"], data_binary[\"label\"], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is : 0.644665461121\n",
      "the best parameter is:  {'svc__C': 1, 'svc__gamma': 0.01, 'tfidfvectorizer__min_df': 3}\n",
      "the score on the test set is:  0.635379061372\n"
     ]
    }
   ],
   "source": [
    "# run non-linear SVC on 3-classifier data\n",
    "\n",
    "vect = TfidfVectorizer(stop_words=\"english\",ngram_range=(1,1),tokenizer=custom_tokenizer,norm=None)\n",
    "param_grid={\n",
    "            'svc__C':[0.01,0.1,1,10],\n",
    "            'svc__gamma':[0.01,0.1,1,10],\n",
    "            'tfidfvectorizer__min_df':[1,2,3]\n",
    "}\n",
    "pipe=make_pipeline(vect,SVC())\n",
    "grid=GridSearchCV(pipe, param_grid,cv=StratifiedKFold(n_splits=10),scoring=\"accuracy\")\n",
    "grid.fit(X_train,y_train)\n",
    "print(\"the best cv score is :\", grid.best_score_)\n",
    "print(\"the best parameter is: \", grid.best_params_)\n",
    "print(\"the score on the test set is: \",grid.score(X_test,y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is : 0.782738095238\n",
      "the best parameter is:  {'svc__C': 1, 'svc__gamma': 0.01, 'tfidfvectorizer__min_df': 3}\n",
      "the score on the test set is:  0.751479289941\n"
     ]
    }
   ],
   "source": [
    "# run non-linear SVC on 2-classifier data\n",
    "\n",
    "vect = TfidfVectorizer(stop_words=\"english\",ngram_range=(1,1),tokenizer=custom_tokenizer,norm=None)\n",
    "param_grid={\n",
    "            'svc__C':[0.01,0.1,1,10],\n",
    "            'svc__gamma':[0.01,0.1,1,10],\n",
    "            'tfidfvectorizer__min_df':[1,2,3]\n",
    "}\n",
    "pipe=make_pipeline(vect,SVC())\n",
    "grid=GridSearchCV(pipe, param_grid,cv=StratifiedKFold(n_splits=10),scoring=\"accuracy\")\n",
    "grid.fit(X_train_binary,y_train_binary)\n",
    "print(\"the best cv score is :\", grid.best_score_)\n",
    "print(\"the best parameter is: \", grid.best_params_)\n",
    "print(\"the score on the test set is: \",grid.score(X_test_binary,y_test_binary) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM does not increase the accuracy score (linear models perform better with high dimensional data?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 4: K nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"label\"], test_size=0.2)\n",
    "X_train_binary,X_test_binary, y_train_binary,y_test_binary=train_test_split(data_binary[\"text\"], data_binary[\"label\"], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is : 0.552441229656\n",
      "the best parameter is:  {'tfidfvectorizer__ngram_range': (1, 2), 'tfidfvectorizer__min_df': 4, 'kneighborsclassifier__n_neighbors': 1}\n",
      "the score on the test set is:  0.595667870036\n"
     ]
    }
   ],
   "source": [
    "#run K nearest neighbor on 3-classifier data\n",
    "\n",
    "vect =TfidfVectorizer(min_df=3, stop_words=\"english\",tokenizer=custom_tokenizer,norm=None)\n",
    "param_grid={\n",
    "    \"tfidfvectorizer__ngram_range\":[(1,1),(1,2),(1,3)],\n",
    "    'tfidfvectorizer__min_df':[1,2,3,4,5],\n",
    "    \"kneighborsclassifier__n_neighbors\":[1,5,10,15,20,30]\n",
    "}\n",
    "pipe=make_pipeline(vect,KNeighborsClassifier())\n",
    "grid=GridSearchCV(pipe,param_grid,cv=StratifiedKFold(n_splits=10),scoring=\"accuracy\")\n",
    "grid.fit(X_train,y_train)\n",
    "print(\"the best cv score is :\", grid.best_score_)\n",
    "print(\"the best parameter is: \", grid.best_params_)\n",
    "print(\"the score on the test set is: \",grid.score(X_test,y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is : 0.729166666667\n",
      "the best parameter is:  {'tfidfvectorizer__ngram_range': (1, 1), 'tfidfvectorizer__min_df': 4, 'kneighborsclassifier__n_neighbors': 1}\n",
      "the score on the test set is:  0.668639053254\n"
     ]
    }
   ],
   "source": [
    "# run K nearest neighbor on 2-classifier data\n",
    "vect =TfidfVectorizer(min_df=3, stop_words=\"english\",tokenizer=custom_tokenizer,norm=None)\n",
    "param_grid={\n",
    "    \"tfidfvectorizer__ngram_range\":[(1,1),(1,2),(1,3)],\n",
    "    'tfidfvectorizer__min_df':[1,2,3,4,5],\n",
    "    \"kneighborsclassifier__n_neighbors\":[1,5,10,15,20,30]\n",
    "}\n",
    "pipe=make_pipeline(vect,KNeighborsClassifier())\n",
    "grid=GridSearchCV(pipe,param_grid,cv=StratifiedKFold(n_splits=10),scoring=\"accuracy\")\n",
    "grid.fit(X_train_binary,y_train_binary)\n",
    "print(\"the best cv score is :\", grid.best_score_)\n",
    "print(\"the best parameter is: \", grid.best_params_)\n",
    "print(\"the score on the test set is: \",grid.score(X_test_binary,y_test_binary) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K nearest neighbors classifier by far has the lowest accuracy scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 5: Nearest Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestCentroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"label\"], test_size=0.2)\n",
    "X_train_binary,X_test_binary, y_train_binary,y_test_binary=train_test_split(data_binary[\"text\"], data_binary[\"label\"], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is : 0.661844484629\n",
      "the best parameter is:  {'tfidfvectorizer__ngram_range': (1, 2), 'tfidfvectorizer__min_df': 1}\n",
      "the score on the test set is:  0.628158844765\n"
     ]
    }
   ],
   "source": [
    "#Run nearest centroid on 3-classifier data\n",
    "\n",
    "vect =TfidfVectorizer( stop_words=\"english\",tokenizer=custom_tokenizer,norm=None)\n",
    "param_grid={\n",
    "    \"tfidfvectorizer__ngram_range\":[(1,1),(1,2),(1,3)],\n",
    "    'tfidfvectorizer__min_df':[1,2,3,4,5]\n",
    "}\n",
    "pipe=make_pipeline(vect,NearestCentroid())\n",
    "grid=GridSearchCV(pipe,param_grid,cv=StratifiedKFold(n_splits=10),scoring=\"accuracy\")\n",
    "grid.fit(X_train,y_train)\n",
    "print(\"the best cv score is :\", grid.best_score_)\n",
    "print(\"the best parameter is: \", grid.best_params_)\n",
    "print(\"the score on the test set is: \",grid.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is : 0.787202380952\n",
      "the best parameter is:  {'tfidfvectorizer__ngram_range': (1, 2), 'tfidfvectorizer__min_df': 2}\n",
      "the score on the test set is:  0.804733727811\n"
     ]
    }
   ],
   "source": [
    "#Run nearest centroid on 2-classifier data\n",
    "\n",
    "vect =TfidfVectorizer( stop_words=\"english\",tokenizer=custom_tokenizer,norm=None)\n",
    "param_grid={\n",
    "    \"tfidfvectorizer__ngram_range\":[(1,1),(1,2),(1,3)],\n",
    "    'tfidfvectorizer__min_df':[1,2,3,4,5]\n",
    "}\n",
    "pipe=make_pipeline(vect,NearestCentroid())\n",
    "grid=GridSearchCV(pipe,param_grid,cv=StratifiedKFold(n_splits=10),scoring=\"accuracy\")\n",
    "grid.fit(X_train_binary,y_train_binary)\n",
    "print(\"the best cv score is :\", grid.best_score_)\n",
    "print(\"the best parameter is: \", grid.best_params_)\n",
    "print(\"the score on the test set is: \",grid.score(X_test_binary,y_test_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the result of NearestCentroid is pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6: SGD + Linear SVM/Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# SGDClassifier is a linear classifiers (SVM, logistic regression, a.o.) with SGD training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"label\"], test_size=0.2)\n",
    "X_train_binary,X_test_binary, y_train_binary,y_test_binary=train_test_split(data_binary[\"text\"], data_binary[\"label\"], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is : 0.679023508137\n",
      "the best parameter is:  {'tfidfvectorizer__ngram_range': (1, 1), 'sgdclassifier__alpha': 0.1, 'tfidfvectorizer__min_df': 2}\n",
      "the score on the test set is:  0.620938628159\n"
     ]
    }
   ],
   "source": [
    "# run SGD + Linear SVM on 3-classifier data\n",
    "\n",
    "vect =TfidfVectorizer(stop_words=\"english\",tokenizer=custom_tokenizer,norm=None)\n",
    "sgd = SGDClassifier(fit_intercept=True,learning_rate='optimal')\n",
    "param_grid={\n",
    "    \"tfidfvectorizer__ngram_range\":[(1,1),(1,2),(1,3)],\n",
    "    'tfidfvectorizer__min_df':[1,2,3,4,5],\n",
    "    \"sgdclassifier__alpha\":[1,0.1,0.01,0.001]\n",
    "}\n",
    "pipe=make_pipeline(vect,sgd)\n",
    "grid=GridSearchCV(pipe,param_grid,cv=StratifiedKFold(n_splits=10),scoring=\"accuracy\")\n",
    "grid.fit(X_train,y_train)\n",
    "print(\"the best cv score is :\", grid.best_score_)\n",
    "print(\"the best parameter is: \", grid.best_params_)\n",
    "print(\"the score on the test set is: \",grid.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is : 0.821428571429\n",
      "the best parameter is:  {'tfidfvectorizer__ngram_range': (1, 2), 'sgdclassifier__alpha': 1, 'tfidfvectorizer__min_df': 1}\n",
      "the score on the test set is:  0.804733727811\n"
     ]
    }
   ],
   "source": [
    "# run SGD + Linear SVM on 2-classifier data\n",
    "\n",
    "vect =TfidfVectorizer(stop_words=\"english\",tokenizer=custom_tokenizer,norm=None)\n",
    "sgd = SGDClassifier(fit_intercept=True,learning_rate='optimal')\n",
    "param_grid={\n",
    "    \"tfidfvectorizer__ngram_range\":[(1,1),(1,2),(1,3)],\n",
    "    'tfidfvectorizer__min_df':[1,2,3,4,5],\n",
    "    \"sgdclassifier__alpha\":[1,0.1,0.01,0.001]\n",
    "}\n",
    "pipe=make_pipeline(vect,sgd)\n",
    "grid=GridSearchCV(pipe,param_grid,cv=StratifiedKFold(n_splits=10),scoring=\"accuracy\")\n",
    "grid.fit(X_train_binary,y_train_binary)\n",
    "print(\"the best cv score is :\", grid.best_score_)\n",
    "print(\"the best parameter is: \", grid.best_params_)\n",
    "print(\"the score on the test set is: \",grid.score(X_test_binary,y_test_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is : 0.679023508137\n",
      "the best parameter is:  {'tfidfvectorizer__ngram_range': (1, 1), 'sgdclassifier__alpha': 0.1, 'tfidfvectorizer__min_df': 2}\n",
      "the score on the test set is:  0.631768953069\n"
     ]
    }
   ],
   "source": [
    "# run SGD + logistic regression on 3-classifier data\n",
    "\n",
    "sgd_lr=SGDClassifier(loss= 'log',fit_intercept=True,learning_rate='optimal')\n",
    "param_grid={\n",
    "    \"tfidfvectorizer__ngram_range\":[(1,1),(1,2),(1,3)],\n",
    "    'tfidfvectorizer__min_df':[1,2,3,4,5],\n",
    "    \"sgdclassifier__alpha\":[1,0.1,0.01,0.001]\n",
    "}\n",
    "pipe=make_pipeline(vect,sgd_lr)\n",
    "grid=GridSearchCV(pipe,param_grid,cv=StratifiedKFold(n_splits=10),scoring=\"accuracy\")\n",
    "grid.fit(X_train,y_train)\n",
    "print(\"the best cv score is :\", grid.best_score_)\n",
    "print(\"the best parameter is: \", grid.best_params_)\n",
    "print(\"the score on the test set is: \",grid.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is : 0.825892857143\n",
      "the best parameter is:  {'tfidfvectorizer__ngram_range': (1, 1), 'sgdclassifier__alpha': 0.1, 'tfidfvectorizer__min_df': 1}\n",
      "the score on the test set is:  0.757396449704\n"
     ]
    }
   ],
   "source": [
    "# run SGD + logistic regression on 2-classifier data\n",
    "\n",
    "sgd_lr=SGDClassifier(loss= 'log',fit_intercept=True,learning_rate='optimal')\n",
    "param_grid={\n",
    "    \"tfidfvectorizer__ngram_range\":[(1,1),(1,2),(1,3)],\n",
    "    'tfidfvectorizer__min_df':[1,2,3,4,5],\n",
    "    \"sgdclassifier__alpha\":[1,0.1,0.01,0.001]\n",
    "}\n",
    "pipe=make_pipeline(vect,sgd_lr)\n",
    "grid=GridSearchCV(pipe,param_grid,cv=StratifiedKFold(n_splits=10),scoring=\"accuracy\")\n",
    "grid.fit(X_train_binary,y_train_binary)\n",
    "print(\"the best cv score is :\", grid.best_score_)\n",
    "print(\"the best parameter is: \", grid.best_params_)\n",
    "print(\"the score on the test set is: \",grid.score(X_test_binary,y_test_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SGD + linear SVM or logistic regression provides slightly lower accuracy than without SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 7: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"label\"], test_size=0.2)\n",
    "X_train_binary,X_test_binary, y_train_binary,y_test_binary=train_test_split(data_binary[\"text\"], data_binary[\"label\"], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is : 0.660940325497\n",
      "the best parameter is:  {'tfidfvectorizer__ngram_range': (1, 2), 'tfidfvectorizer__min_df': 1, 'multinomialnb__alpha': 10}\n",
      "the score on the test set is:  0.660649819495\n"
     ]
    }
   ],
   "source": [
    "# 3-classifier\n",
    "\n",
    "vect =TfidfVectorizer(stop_words=\"english\",tokenizer=custom_tokenizer,norm=None)\n",
    "param_grid={\n",
    "    \"tfidfvectorizer__ngram_range\":[(1,1),(1,2),(1,3)],\n",
    "    'tfidfvectorizer__min_df':[1,2,3,4,5],\n",
    "    \"multinomialnb__alpha\":[0.1,1,10]\n",
    "}\n",
    "pipe=make_pipeline(vect,MultinomialNB())\n",
    "grid=GridSearchCV(pipe,param_grid,cv=StratifiedKFold(n_splits=10),scoring=\"accuracy\")\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "print(\"the best parameter is: \", grid.best_params_)\n",
    "print(\"the score on the test set is: \",grid.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is : 0.809523809524\n",
      "the best parameter is:  {'tfidfvectorizer__ngram_range': (1, 2), 'tfidfvectorizer__min_df': 1, 'multinomialnb__alpha': 10}\n",
      "the score on the test set is:  0.786982248521\n"
     ]
    }
   ],
   "source": [
    "#2-classifier\n",
    "\n",
    "vect =TfidfVectorizer(stop_words=\"english\",tokenizer=custom_tokenizer,norm=None)\n",
    "param_grid={\n",
    "    \"tfidfvectorizer__ngram_range\":[(1,1),(1,2),(1,3)],\n",
    "    'tfidfvectorizer__min_df':[1,2,3,4,5],\n",
    "    \"multinomialnb__alpha\":[0.1,1,10]\n",
    "}\n",
    "pipe=make_pipeline(vect,MultinomialNB())\n",
    "grid=GridSearchCV(pipe,param_grid,cv=StratifiedKFold(n_splits=10),scoring=\"accuracy\")\n",
    "grid.fit(X_train_binary,y_train_binary)\n",
    "print(\"the best cv score is :\", grid.best_score_)\n",
    "print(\"the best parameter is: \", grid.best_params_)\n",
    "print(\"the score on the test set is: \",grid.score(X_test_binary,y_test_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is : 0.649186256781\n",
      "the best parameter is:  {'tfidfvectorizer__ngram_range': (1, 2), 'bernoullinb__alpha': 0.1, 'tfidfvectorizer__min_df': 1}\n",
      "the score on the test set is:  0.649819494585\n"
     ]
    }
   ],
   "source": [
    "#3-classifier \n",
    "\n",
    "vect =TfidfVectorizer(stop_words=\"english\",tokenizer=custom_tokenizer,norm=None)\n",
    "param_grid={\n",
    "    \"tfidfvectorizer__ngram_range\":[(1,1),(1,2),(1,3)],\n",
    "    'tfidfvectorizer__min_df':[1,2,3,4,5],\n",
    "    \"bernoullinb__alpha\":[0.1,1,10]\n",
    "}\n",
    "pipe=make_pipeline(vect,BernoulliNB())\n",
    "grid=GridSearchCV(pipe,param_grid,cv=StratifiedKFold(n_splits=10),scoring=\"accuracy\")\n",
    "grid.fit(X_train,y_train)\n",
    "print(\"the best cv score is :\", grid.best_score_)\n",
    "print(\"the best parameter is: \", grid.best_params_)\n",
    "print(\"the score on the test set is: \",grid.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best cv score is : 0.799107142857\n",
      "the best parameter is:  {'tfidfvectorizer__ngram_range': (1, 1), 'bernoullinb__alpha': 1, 'tfidfvectorizer__min_df': 2}\n",
      "the score on the test set is:  0.798816568047\n"
     ]
    }
   ],
   "source": [
    "#3-classifier \n",
    "\n",
    "vect =TfidfVectorizer(stop_words=\"english\",tokenizer=custom_tokenizer,norm=None)\n",
    "param_grid={\n",
    "    \"tfidfvectorizer__ngram_range\":[(1,1),(1,2),(1,3)],\n",
    "    'tfidfvectorizer__min_df':[1,2,3,4,5],\n",
    "    \"bernoullinb__alpha\":[0.1,1,10]\n",
    "}\n",
    "pipe=make_pipeline(vect,BernoulliNB())\n",
    "grid=GridSearchCV(pipe,param_grid,cv=StratifiedKFold(n_splits=10),scoring=\"accuracy\")\n",
    "grid.fit(X_train_binary,y_train_binary)\n",
    "print(\"the best cv score is :\", grid.best_score_)\n",
    "print(\"the best parameter is: \", grid.best_params_)\n",
    "print(\"the score on the test set is: \",grid.score(X_test_binary,y_test_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- naive bayes is performing well on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Combining model results (not used in paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The best results comes from logistic regression, linear SVC, multinomial naive bayes, and Bernolli naive bayes. Combine the result of these models and take the majority vote. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the parameters tuned before to predict training data by 4 models (if it's 2 vs 2, then it goes by the result from linear SVC)\n",
    "\n",
    "# create a new dataframe\n",
    "combined_train = pd.DataFrame(data[\"text\"][:650])\n",
    "combined_train[\"label\"]=data[\"label\"][:650]\n",
    "\n",
    "combined_test = pd.DataFrame(data[\"text\"][651:])\n",
    "combined_test[\"label\"]=data[\"label\"][651:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are all u adults ready for your onslaught of vaccines? Say NO to Vaccine Mandate! NoMandates</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kagro in the Morning: on Chapel Hill; vax roundup; King collapse; new AUMF state secrets</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CANCER VACCINES??!!?!!?</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mexico's measles vaccination rate? 99%. The US'? 92%. Why immigrants aren't behind measles:</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vaccine stories not compelling without: aborted fetuses homosexuality autism Hitler fascism conspiracy promiscuity coverup U.N.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                              text  \\\n",
       "0                                     Are all u adults ready for your onslaught of vaccines? Say NO to Vaccine Mandate! NoMandates   \n",
       "1                                         Kagro in the Morning: on Chapel Hill; vax roundup; King collapse; new AUMF state secrets   \n",
       "3                                                                                                          CANCER VACCINES??!!?!!?   \n",
       "4                                      Mexico's measles vaccination rate? 99%. The US'? 92%. Why immigrants aren't behind measles:   \n",
       "5  Vaccine stories not compelling without: aborted fetuses homosexuality autism Hitler fascism conspiracy promiscuity coverup U.N.   \n",
       "\n",
       "   label  \n",
       "0   -1.0  \n",
       "1    0.0  \n",
       "3    0.0  \n",
       "4    0.0  \n",
       "5    1.0  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>Illinois announced EMERGENCY voting on measles vaccine for all, no exemptions The vote is tomorrow. you know, we... ht</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>Author of controversial vaccination/autism study shunned by Salem via</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>funnyordie: clippership: Finally! A safe option for anti-vaxxers. antivax</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>autism vax pharma Excellent. The audacity of pharma and MD's that say they know more about the mothers child...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>Africa reaches 6 months w/out a case of polio. We must continue to reach every child w/ the polio vaccine. endpolio</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                       text  \\\n",
       "673  Illinois announced EMERGENCY voting on measles vaccine for all, no exemptions The vote is tomorrow. you know, we... ht   \n",
       "674                                                   Author of controversial vaccination/autism study shunned by Salem via   \n",
       "675                                               funnyordie: clippership: Finally! A safe option for anti-vaxxers. antivax   \n",
       "676         autism vax pharma Excellent. The audacity of pharma and MD's that say they know more about the mothers child...   \n",
       "677     Africa reaches 6 months w/out a case of polio. We must continue to reach every child w/ the polio vaccine. endpolio   \n",
       "\n",
       "     label  \n",
       "673    0.0  \n",
       "674    0.0  \n",
       "675    1.0  \n",
       "676   -1.0  \n",
       "677    0.0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill in the result of prediction by these models using the best parameters got above\n",
    "\n",
    "#first, logistic regression\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range= (1,2), min_df=1, stop_words=\"english\",tokenizer=custom_tokenizer,norm=None).fit(combined_train[\"text\"])\n",
    "train_vect = vect.transform(combined_train[\"text\"])\n",
    "\n",
    "logreg=LogisticRegression(C=0.01)\n",
    "logreg.fit(train_vect,combined_train[\"label\"])\n",
    "\n",
    "test_vect=vect.transform(combined_test[\"text\"])\n",
    "logreg_result = logreg.predict(test_vect)\n",
    "combined_test[\"logreg_result\"]=logreg_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# second, linear SVC\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range= (1,1), min_df=1, stop_words=\"english\",tokenizer=custom_tokenizer,norm=None).fit(combined_train[\"text\"])\n",
    "train_vect=vect.transform(combined_train[\"text\"])\n",
    "\n",
    "svc = LinearSVC(C=0.001)\n",
    "svc.fit(train_vect,combined_train['label'])\n",
    "\n",
    "test_vect=vect.transform(combined_test['text'])\n",
    "svc_result=svc.predict(test_vect)\n",
    "combined_test[\"svc_result\"]=svc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# third, multinomial naive bayes\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range= (1,2), min_df=1, stop_words=\"english\",tokenizer=custom_tokenizer,norm=None).fit(combined_train[\"text\"])\n",
    "train_vect=vect.transform(combined_train[\"text\"])\n",
    "\n",
    "multinomialNB = MultinomialNB(alpha=10)\n",
    "multinomialNB.fit(train_vect,combined_train['label'])\n",
    "\n",
    "test_vect=vect.transform(combined_test['text'])\n",
    "multinomialNB_result=multinomialNB.predict(test_vect)\n",
    "combined_test[\"multinomialNB_result\"]=multinomialNB_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# last, Bernoulli naive bayes\n",
    "\n",
    "bernoulliNB=BernoulliNB(alpha=10)\n",
    "bernoulliNB.fit(train_vect,combined_train['label'])\n",
    "\n",
    "bernoulliNB_result=bernoulliNB.predict(test_vect)\n",
    "combined_test[\"bernoulliNB_result\"]=bernoulliNB_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>logreg_result</th>\n",
       "      <th>svc_result</th>\n",
       "      <th>multinomialNB_result</th>\n",
       "      <th>bernoulliNB_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>Illinois announced EMERGENCY voting on measles vaccine for all, no exemptions The vote is tomorrow. you know, we... ht</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>Author of controversial vaccination/autism study shunned by Salem via</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>funnyordie: clippership: Finally! A safe option for anti-vaxxers. antivax</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>autism vax pharma Excellent. The audacity of pharma and MD's that say they know more about the mothers child...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>Africa reaches 6 months w/out a case of polio. We must continue to reach every child w/ the polio vaccine. endpolio</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>The Anti-Vaccine Generation: How Movement Against Shots Got Its Start via</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>Powerful Anti-HIV Agent Can Work in a Vaccine</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>No, only that I veered a bit from specific vaccine discussion, but you did that yourself too.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>Ebola vaccine trial to start on volunteers in Liberia</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>How vaccinations may have opened up a new front in the GOP culture wars: (John Locher/AP)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                       text  \\\n",
       "673  Illinois announced EMERGENCY voting on measles vaccine for all, no exemptions The vote is tomorrow. you know, we... ht   \n",
       "674                                                   Author of controversial vaccination/autism study shunned by Salem via   \n",
       "675                                               funnyordie: clippership: Finally! A safe option for anti-vaxxers. antivax   \n",
       "676         autism vax pharma Excellent. The audacity of pharma and MD's that say they know more about the mothers child...   \n",
       "677     Africa reaches 6 months w/out a case of polio. We must continue to reach every child w/ the polio vaccine. endpolio   \n",
       "678                                               The Anti-Vaccine Generation: How Movement Against Shots Got Its Start via   \n",
       "679                                                                           Powerful Anti-HIV Agent Can Work in a Vaccine   \n",
       "681                           No, only that I veered a bit from specific vaccine discussion, but you did that yourself too.   \n",
       "683                                                                   Ebola vaccine trial to start on volunteers in Liberia   \n",
       "684                               How vaccinations may have opened up a new front in the GOP culture wars: (John Locher/AP)   \n",
       "\n",
       "     label  logreg_result  svc_result  multinomialNB_result  \\\n",
       "673    0.0            1.0         1.0                   1.0   \n",
       "674    0.0           -1.0        -1.0                  -1.0   \n",
       "675    1.0            1.0         1.0                   1.0   \n",
       "676   -1.0           -1.0        -1.0                  -1.0   \n",
       "677    0.0            0.0        -1.0                   1.0   \n",
       "678    1.0            1.0         1.0                   1.0   \n",
       "679    0.0            0.0         0.0                   0.0   \n",
       "681    0.0            1.0         1.0                   1.0   \n",
       "683    0.0            0.0         0.0                   0.0   \n",
       "684    1.0            1.0         1.0                   0.0   \n",
       "\n",
       "     bernoulliNB_result  \n",
       "673                 0.0  \n",
       "674                 0.0  \n",
       "675                 0.0  \n",
       "676                 0.0  \n",
       "677                 0.0  \n",
       "678                 0.0  \n",
       "679                 0.0  \n",
       "681                 0.0  \n",
       "683                 0.0  \n",
       "684                 0.0  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result=combined_test.iloc[:,2:].mode(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1\n",
       "673  1.0  NaN\n",
       "674 -1.0  NaN\n",
       "675  1.0  NaN\n",
       "676 -1.0  NaN\n",
       "677  0.0  NaN\n",
       "678  1.0  NaN\n",
       "679  0.0  NaN\n",
       "681  1.0  NaN\n",
       "683  0.0  NaN\n",
       "684  1.0  NaN\n",
       "685  1.0  NaN\n",
       "686 -1.0  NaN\n",
       "687  1.0  NaN\n",
       "688  0.0  NaN\n",
       "689  1.0  NaN\n",
       "690 -1.0  NaN\n",
       "692  0.0  NaN\n",
       "693  0.0  NaN\n",
       "694 -1.0  NaN\n",
       "695  1.0  NaN\n",
       "696 -1.0  NaN\n",
       "697  0.0  NaN\n",
       "698  1.0  NaN\n",
       "699  1.0  NaN\n",
       "700  1.0  NaN\n",
       "701  1.0  NaN\n",
       "702  1.0  NaN\n",
       "703  1.0  NaN\n",
       "704  0.0  NaN\n",
       "705  1.0  NaN\n",
       "706  1.0  NaN\n",
       "707  0.0  NaN\n",
       "708  0.0  NaN\n",
       "709  1.0  NaN\n",
       "710  1.0  NaN\n",
       "711  0.0  NaN\n",
       "712 -1.0  NaN\n",
       "713 -1.0  NaN\n",
       "714  0.0  NaN\n",
       "715  0.0  NaN\n",
       "716  1.0  NaN\n",
       "717  0.0  NaN\n",
       "718  1.0  NaN\n",
       "719 -1.0  NaN\n",
       "720  1.0  NaN\n",
       "721  0.0  NaN\n",
       "722  0.0  NaN\n",
       "723  0.0  NaN\n",
       "724  0.0  NaN\n",
       "725  1.0  NaN\n",
       "..   ...  ...\n",
       "579  1.0  NaN\n",
       "580  0.0  NaN\n",
       "581  1.0  NaN\n",
       "582  1.0  NaN\n",
       "583  1.0  NaN\n",
       "584  0.0  NaN\n",
       "585  1.0  NaN\n",
       "586  1.0  NaN\n",
       "587  0.0  NaN\n",
       "588  0.0  NaN\n",
       "589  0.0  NaN\n",
       "590  0.0  NaN\n",
       "591  1.0  NaN\n",
       "592  1.0  NaN\n",
       "593  0.0  NaN\n",
       "594  0.0  NaN\n",
       "595  0.0  NaN\n",
       "596  0.0  NaN\n",
       "597  0.0  NaN\n",
       "598 -1.0  NaN\n",
       "599  1.0  NaN\n",
       "600 -1.0  NaN\n",
       "601  0.0  NaN\n",
       "602  0.0  NaN\n",
       "603 -1.0  NaN\n",
       "604  0.0  NaN\n",
       "605  1.0  NaN\n",
       "606 -1.0  NaN\n",
       "607  1.0  NaN\n",
       "608  0.0  NaN\n",
       "609  1.0  NaN\n",
       "610  1.0  NaN\n",
       "611 -1.0  NaN\n",
       "612  0.0  NaN\n",
       "613  1.0  NaN\n",
       "614  1.0  NaN\n",
       "615  0.0  NaN\n",
       "616  1.0  NaN\n",
       "617  1.0  NaN\n",
       "618  0.0  NaN\n",
       "619  0.0  NaN\n",
       "620 -1.0  1.0\n",
       "621  1.0  NaN\n",
       "622  0.0  NaN\n",
       "623  0.0  NaN\n",
       "624  0.0  NaN\n",
       "625 -1.0  NaN\n",
       "626  1.0  NaN\n",
       "627  0.0  NaN\n",
       "628  1.0  NaN\n",
       "\n",
       "[732 rows x 2 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=0\n",
    "for n in range(len(result)):\n",
    "    if np.isnan(result.iloc[n,1]):\n",
    "        result.set_value(n,0,combined_test.iloc[n,3])\n",
    "    n = n + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_test[\"majority_vote\"]=result[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "250px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "601px",
    "left": "0px",
    "right": "auto",
    "top": "105px",
    "width": "249px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
